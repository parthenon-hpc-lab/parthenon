image: cuda11.3-mpi-hdf5

# Is performed before the scripts in the stages step
before_script:
    - export OMP_PROC_BIND=close
    - export OMP_PLACES=cores
    - export OMP_NUM_THREADS=1
    - export CTEST_OUTPUT_ON_FAILURE=1
    - export MACHINE_CFG=${PWD}/cmake/machinecfg/CI.cmake
    - export J=$(( $(nproc --all) / 4  + 1 )) && echo Using ${J} cores during build

cache:
  paths:
    - tst/regression/gold_standard/

variables:
  GIT_SUBMODULE_STRATEGY: recursive
stages:
  - short
  - performance_and_regression
  - coverage
  - style

style-check:
  tags:
    - cpu
  stage: style
  script:
    - python ./tst/style/cpplint.py --counting=detailed --recursive src example tst

copyright-check:
  tags:
    - cpu
  stage: style
  script:
    - mkdir build-copyright-check
    - cd build-copyright-check
    - cmake -DMACHINE_VARIANT=mpi
      -DCMAKE_CXX_FLAGS=-Werror
      ../
    - make check-copyright
  artifacts:
    when: always
    expire_in: 3 days
    paths:
      - build-copyright-check/CMakeFiles/CMakeOutput.log

parthenon-cuda-unit:
  tags:
    - cuda
  stage: short
  script:
    - mkdir build-cuda-debug
    - cd build-cuda-debug
    - cmake -DCMAKE_BUILD_TYPE=Debug
      -DMACHINE_VARIANT=cuda-mpi
      -DCMAKE_CXX_FLAGS=-Werror
      ../
    - make -j${J}
    - ctest -LE 'performance|regression'
  artifacts:
    when: always
    expire_in: 3 days
    paths:
      - build-cuda-debug/CMakeFiles/CMakeOutput.log

# run unit suite on CPUs
parthenon-cpu-unit:
  tags:
    - cpu
  stage: short
  script:
    - mkdir build-debug
    - cd build-debug
    - cmake -DCMAKE_BUILD_TYPE=Debug
      -DMACHINE_VARIANT=mpi
      ../
    - make -j${J}
    - ctest -LE "performance|regression"
  artifacts:
    when: always
    expire_in: 3 days
    paths:
      - build-debug/CMakeFiles/CMakeOutput.log

parthenon-cuda-short:
  tags:
    - cuda
  stage: short
  script:
    - mkdir build-cuda-perf-mpi
    - cd build-cuda-perf-mpi
    - cmake -DCMAKE_BUILD_TYPE=Release
      -DMACHINE_VARIANT=cuda-mpi
      ../
    - make -j${J} advection-example
    - export OMPI_MCA_mpi_common_cuda_event_max=1000
    # Run one example with variables
    - ctest -R regression_mpi_test:output_hdf5
    # and one that uses swarms
    - make -j${J} particle-leapfrog
    - ctest -R regression_mpi_test:particle_leapfrog
    # Now testing if there are no hidden memcopies between host and device.
    # Using a static grid (i.e., not AMR) as additional transfers are expected
    # during loadbalance and refinement, but not for a static grid. We also need to
    # turn off sparse, since sparse results in additional transfers.
    # Also delaying start as there are explicit copies during initialization, e.g.,
    # when the Variable caches are created.
    - nsys profile --delay=5 --duration=5 --stats=true -s none -t cuda example/advection/advection-example
      -i ../tst/regression/test_suites/advection_performance/parthinput.advection_performance
      parthenon/mesh/nx1=128 parthenon/mesh/nx2=128  parthenon/mesh/nx3=128
      parthenon/meshblock/nx1=64 parthenon/meshblock/nx2=64 parthenon/meshblock/nx3=64
      parthenon/sparse/enable_sparse=false parthenon/time/nlim=2000 parthenon/time/tlim=2 |& tee profile.txt
    - test $(grep HtoD profile.txt |wc -l) == 0
    - test $(grep DtoH profile.txt |wc -l) == 0
  artifacts:
    when: always
    expire_in: 3 days
    paths:
      - build-cuda-perf-mpi/CMakeFiles/CMakeOutput.log
      - build-cuda-perf-mpi/profile.txt

parthenon-cpu-short:
  tags:
    - cpu
  stage: short
  script:
    - mkdir build-perf-mpi
    - cd build-perf-mpi
    - cmake -DCMAKE_BUILD_TYPE=Release
      -DMACHINE_VARIANT=mpi
      ../
    # Run one example with variables
    - make -j${J} advection-example
    - ctest -R regression_mpi_test:output_hdf5
    # and one that uses swarms
    - make -j${J} particle-leapfrog
    - ctest -R regression_mpi_test:particle_leapfrog
  artifacts:
    when: always
    expire_in: 3 days
    paths:
      - build-perf-mpi/CMakeFiles/CMakeOutput.log

parthenon-cuda-performance_and_regression:
  only:
    - schedules
    - develop
    - web
  tags:
    - cuda
  stage: performance_and_regression
  script:
    - mkdir build-cuda-perf
    - cd build-cuda-perf
    - cmake -DCMAKE_BUILD_TYPE=Release
      -DPARTHENON_DISABLE_MPI=ON
      -DMACHINE_VARIANT=cuda
      ../
    - make -j${J}
    - ctest -L "performance|regression" -LE "mpi-yes|perf-reg"
  artifacts:
    when: always
    expire_in: 3 days
    paths:
      - build-cuda-perf/CMakeFiles/CMakeOutput.log
      - build-cuda-perf/tst/regression/outputs/advection_convergence/advection-errors.dat
      - build-cuda-perf/tst/regression/outputs/advection_convergence/advection-errors.png

parthenon-cuda-performance_and_regression-mpi:
  only:
    - schedules
    - develop
    - web
  tags:
    - cuda
  stage: performance_and_regression
  script:
    - mkdir -p build-cuda-perf-mpi
    - cd build-cuda-perf-mpi
    - cmake -DCMAKE_BUILD_TYPE=Release
      -DMACHINE_VARIANT=cuda-mpi
      ../
    - make -j${J}
    - export OMPI_MCA_mpi_common_cuda_event_max=1000
#    - ctest -L "performance" # no need for performance test as currently none use MPI
    - ctest -L regression -LE "mpi-no|perf-reg" --timeout 3600
  artifacts:
    when: always
    expire_in: 3 days
    paths:
      - build-cuda-perf-mpi/CMakeFiles/CMakeOutput.log
      - build-cuda-perf-mpi/tst/regression/outputs/advection_convergence_mpi/advection-errors.dat
      - build-cuda-perf-mpi/tst/regression/outputs/advection_convergence_mpi/advection-errors.png

# run performance and regression on CPUs without MPI
parthenon-cpu-performance_and_regression:
  only:
    - schedules
    - develop
    - web
  tags:
    - cpu
  stage: performance_and_regression
  script:
    - mkdir build-perf
    - cd build-perf
    - cmake -DCMAKE_BUILD_TYPE=Release
      -DPARTHENON_DISABLE_MPI=ON
      ../
    - make -j${J}
    - ctest -L "performance|regression" -LE "mpi-yes|perf-reg"
  artifacts:
    when: always
    expire_in: 3 days
    paths:
      - build-perf/CMakeFiles/CMakeOutput.log
      - build-perf/tst/regression/outputs/advection_convergence/advection-errors.dat
      - build-perf/tst/regression/outputs/advection_convergence/advection-errors.png

# run performance and regression on CPUs with MPI
parthenon-cpu-performance_and_regression-mpi:
  only:
    - schedules
    - develop
    - web
  tags:
    - cpu
  stage: performance_and_regression
  script:
    - mkdir -p build-perf-mpi
    - cd build-perf-mpi
    - cmake -DCMAKE_BUILD_TYPE=Release
      -DMACHINE_VARIANT=mpi
      ../
    - make -j${J}
#    - ctest -L "performance" # no need for performance test as currently none use MPI
    - ctest -L regression -LE "mpi-no|perf-reg"
  artifacts:
    when: always
    expire_in: 3 days
    paths:
      - build-perf-mpi/CMakeFiles/CMakeOutput.log
      - build-perf-mpi/tst/regression/outputs/advection_convergence_mpi/advection-errors.dat
      - build-perf-mpi/tst/regression/outputs/advection_convergence_mpi/advection-errors.png

# run unit suite on CPUs with code coverage
parthenon-cpu-coverage:
  only:
    - schedules
    - develop
    - web
  tags:
    - cpu
  stage: coverage
  script:
    - mkdir build-debug-coverage
    - cd build-debug-coverage
    - cmake -DCMAKE_BUILD_TYPE=Debug
      -DCODE_COVERAGE=ON
      -DMACHINE_VARIANT=mpi
      ../ && make -j${J} && make coverage && make coverage-upload
  artifacts:
    when: always
    expire_in: 3 days
    paths:
      - build-debug-coverage/CMakeFiles/CMakeOutput.log

